from typing import Optional, Tuple, Union
import torch
import math, warnings
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
import numpy as np

def pair(t):
    return t if isinstance(t, tuple) or isinstance(t, list) else (t, t)

def triplet(t):
    return t if isinstance(t, tuple) or isinstance(t, list) else (t, t, t)

def build_1d_sincos_posemb(max_seq_len: int, embed_dim: int=1024):
    """ Build positional embedding matrix with `batch_first`.
    
    Parameters
    ----------
    max_seq_len: int
        Maximum length of the input sequence.

    embed_dim: int, default=1024
        Dimension of each token.
    
    Returns
    ----------
    pe: torch.Tensor with shape (1, max_seq_length, embed_dim)
    """
    if embed_dim % 2 != 0:
        raise ValueError("Cannot use sin/cos positional encoding with "
                         "odd dim (got dim={:d})".format(embed_dim))
    pe = torch.zeros(max_seq_len, embed_dim)
    position = torch.arange(0, max_seq_len).unsqueeze(1)
    div_term = torch.exp((torch.arange(0, embed_dim, 2, dtype=torch.float) *
                          -(math.log(10000.0) / embed_dim)))
    pe[:, 0::2] = torch.sin(position.float() * div_term)
    pe[:, 1::2] = torch.cos(position.float() * div_term)
    pe = pe[None, :, :]
    return pe


def build_2d_sincos_posemb(h, w, embed_dim=1024, temperature=10000.):
    """Sine-cosine positional embeddings from MoCo-v3

    Source: https://github.com/facebookresearch/moco-v3/blob/main/vits.py
    """
    grid_w = torch.arange(w, dtype=torch.float32)
    grid_h = torch.arange(h, dtype=torch.float32)
    grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing='ij')
    assert embed_dim % 4 == 0, 'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'
    pos_dim = embed_dim // 4
    omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim
    omega = 1. / (temperature ** omega)
    out_w = torch.einsum('m,d->md', [grid_w.flatten(), omega])
    out_h = torch.einsum('m,d->md', [grid_h.flatten(), omega])
    pos_emb = torch.cat([torch.sin(out_w), torch.cos(out_w), torch.sin(out_h), torch.cos(out_h)], dim=1)[None, :, :]
    pos_emb = rearrange(pos_emb, 'b (h w) d -> b d h w', h=h, w=w, d=embed_dim)
    return pos_emb


def build_3d_sincos_posemb(h, w, d, embed_dim=1024, temperature=10000.):
    """Sine-cosine positional embeddings from MoCo-v3 (adapted to work with any embed_dim)

    Source: https://github.com/facebookresearch/moco-v3/blob/main/vits.py and
    https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py
    """
    grid_w = torch.arange(w, dtype=torch.float32)
    grid_h = torch.arange(h, dtype=torch.float32)
    grid_d = torch.arange(d, dtype=torch.float32)
    grid_w, grid_h, grid_d = torch.meshgrid(grid_w, grid_h, grid_d, indexing='ij')
    # Trick from https://github.com/tatp22/multidim-positional-encoding
    # We take a larger 'embed_dim' if not divisible by 6 
    pos_dim = int(np.ceil(embed_dim / 6))
    omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim
    omega = 1. / (temperature ** omega)
    out_w = torch.einsum('m,d->md', [grid_w.flatten(), omega])
    out_h = torch.einsum('m,d->md', [grid_h.flatten(), omega])
    out_d = torch.einsum('m,d->md', [grid_d.flatten(), omega])
    pos_emb = torch.cat([torch.sin(out_w), torch.cos(out_w), 
                         torch.sin(out_h), torch.cos(out_h), 
                         torch.sin(out_d), torch.cos(out_d)], dim=1)[None, :, :] # add batch dim
    pos_emb = rearrange(pos_emb[:, :, :embed_dim], 'b (h w d) l -> b l h w d', # truncate along the embedding dimension
                        h=h, w=w, d=d, l=embed_dim)
    return pos_emb

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class FeaturesInputAdapter(nn.Module):
    """Adapter for 1D vectors, like tabular data.
     Creates tokens from each component over the 1D vector.
     It was introduced in [1] as "Feature Tokenizer".
     [1] Revisiting Deep Learning Models for Tabular Data, NeurIPS 2021
     """

    def __init__(self,
                 n_features: int,
                 dim_tokens: int):
        super().__init__()
        self.n_features = n_features
        self.dim_tokens = dim_tokens
        self.weight = nn.Parameter(torch.randn(self.n_features, self.dim_tokens))
        self.bias = nn.Parameter(torch.randn(self.n_features, self.dim_tokens))

    def forward(self, x):
        """Forward pass through input adapter, transforming 1D vector to sequence of tokens.
        :param x: Input tabular tensor, shape (*, `n_features`)
        Returns tensor of shape (*, `n_features`, `dim_tokens`)
        """
        if x.ndim == 2:
            x = rearrange(x, 'b n -> b n 1')
        elif x.ndim == 3:
            x = rearrange(x, 'b n l-> b (n l) 1')
        else:
            raise ValueError(f"Unexpected dim: {x.shape}")
        # Features -> tokens projection
        x = x * self.weight + self.bias
        return x


class SimpleFeaturesInputAdapter(nn.Module):
    """Adapter for 1D vectors, like tabular data without learnable weights.
    It directly maps the input vector to a 1-length sequence
    """
    def forward(self, x):
        assert x.ndim == 2, f"Expected 2D tensors, got {x.shape}"
        x = rearrange(x, 'b n -> b 1 n')
        return x


class PatchEmbeddings(nn.Module):
    """Converts 3D volume to a sequence of patch embeddings.
    Optionally, it adds positional embeddings to the sequence of patch embeddings. 
    This is useful for ViT-based models (e.g. DINO, DINOv2, I-JEPA)
    
    Parameters
    ----------
    num_channels: int
        Number of input channels of the volume/feature map

    patch_size: int or tuple of (int, int, int)
        Patch size over the volume size.
    
    embed_dim: None or int, default=768
        Dimension of output patch embeddings.
    
    add_pos_emb: bool, default=True
        Set to True to add positional embeddings. 

    sincos_pos_emb: bool, default=True
        If True, set 3d sin-cos positional embeddings init.

    learnable_pos_emb: bool, default=False
        If True, learns positional embeddings when using sin-cos initialization.
        It is automatically learned if sin-cos initialization is disabled.

    volume_size: None, int or (int, int, int), default=None
        Volume size. Used to initialize the positional embeddings.
        It must be set if 'add_pos_embed' is True. 
    """
    def __init__(self,
                 num_channels: int = 1,
                 patch_size: Union[int, Tuple[int,int,int]] = 16,
                 embed_dim: int = 768,
                 add_pos_embed: bool = True,
                 sincos_pos_emb: bool = True,
                 learnable_pos_emb: bool = False,
                 image_size: Union[None, int, Tuple[int,int,int]] = 128):

        super().__init__()
        self.num_channels = num_channels
        self.patch_size = triplet(patch_size)
        self.embed_dim = embed_dim
        self.add_pos_emb = add_pos_embed
        self.sincos_pos_emb = sincos_pos_emb
        self.learnable_pos_emb = learnable_pos_emb
        self.image_size = triplet(image_size)

        # Patch height, width, and depth
        self.P_H = max(1, self.patch_size[0])
        self.P_W = max(1, self.patch_size[1])
        self.P_D = max(1, self.patch_size[2])

        if self.add_pos_emb:
            # Fixed-size positional embeddings. Can be interpolated to different input sizes
            self.N_H = self.image_size[0] // self.P_H
            self.N_W = self.image_size[1] // self.P_W
            self.N_D = self.image_size[2] // self.P_D
            if self.sincos_pos_emb:
                self.pos_emb = build_3d_sincos_posemb(h=self.N_H, w=self.N_W, d=self.N_D, 
                                                      embed_dim=self.embed_dim)
                self.pos_emb = nn.Parameter(self.pos_emb, requires_grad=self.learnable_pos_emb)
            else:
                self.pos_emb = nn.Parameter(torch.zeros(1, self.embed_dim, self.N_H, self.N_W, self.N_D))
                trunc_normal_(self.pos_emb, std=0.02)

        # Image -> patch embeddings projection
        self.proj = nn.Conv3d(
                in_channels=self.num_channels, out_channels=self.embed_dim,
                kernel_size=(self.P_H, self.P_W, self.P_D), stride=(self.P_H, self.P_W, self.P_D)
            )
        

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_emb'}

    def forward(self, x: torch.Tensor):
        """
        Transform input 3d volume to sequence of patch embeddings.
        
        Parameters
        ----------
        x: torch.Tensor, shape (batch_size, in_channels, height, width, depth)
            Input volume.

        Returns
        ----------
        x: torch.Tensor, shape (batch_size, num_patches, embed_dim)
        """
        _, _, H, W, D = x.shape
        assert (H % self.P_H == 0) and (W % self.P_W == 0) and (D % self.P_D ==0), \
            f'Image sizes {H}x{W}x{D} must be divisible by patch sizes {self.P_H}x{self.P_W}x{self.P_D}'
        N_H, N_W, N_D = H // self.P_H, W // self.P_W, D // self.P_D # Number of patches in height, width and depth

        # Create patch embeddings [B, C, H, W, D] -> [B, (H*WxD), C]
        x = rearrange(self.proj(x), 'b d nh nw nd -> b (nh nw nd) d')

        if self.add_pos_emb:
            # Add positional embedding
            if self.N_H != N_H or self.N_W != N_W or self.N_D != N_D:
                # New resolution of the input -> needs to interpolate
                x_pos_emb = F.interpolate(self.pos_emb, size=(N_H, N_W, N_D), 
                                          mode='trilinear', align_corners=False)
            else:
                x_pos_emb = self.pos_emb
            x = x + rearrange(x_pos_emb, 'b d nh nw nd -> b (nh nw nd) d')

        return x